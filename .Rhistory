require(stringr)
require(tidyverse)
# Set your working direct
setwd("/media/simon/Data/Dropbox/Scripts/R/SWOWGIT/git")
# Determine unknown and missing response tokens
unknown.Token = 'Unknown word'
missing.Token = 'No more responses'
listlength.Min = 14
listlength.Max = 18
age.Min = 16
# Participants who tested the experiments (will be excluded)
testsubjects = c(1,2,71,7334,7336,36869,60804,76083,76308,83324,89552,89569,99569,100429,112713,122019,122857)
dirname(parent.frame(2)$ofile
)
dirname(parent.frame(2)$ofile)
library(rstudioapi) # load it
current_path = getActiveDocumentContext()$path
setwd(dirname(current_path ))
print( getwd() )
message('Current working directory: ', getwd() )
require(rstudioapi)
require(stringr)
require(tidyverse)
rm(list = ls())
# Set your working direct
current_path = getActiveDocumentContext()$path
setwd(dirname(current_path ))
message('Current working directory: ', getwd() )
# Determine unknown and missing response tokens
unknown.Token = 'Unknown word'
missing.Token = 'No more responses'
listlength.Min = 14
listlength.Max = 18
listlength.default = 14
age.Min        = 16
# Participants who tested the experiments (will be excluded)
testsubjects = c(1,2,71,7334,7336,36869,60804,76083,76308,83324,89552,89569,99569,100429,112713,122019,122857)
# Languages considered native
nativeLanguages = c('United States','Canada','Australia','New Zealand','Puerto Rico','Ireland',
'United Kingdom','South Africa','Jamaica')
responseCountTreshold = 300
# Criteria for removing participants with
# 1. over 60% missing or unknown responses
criteria.X = 0.6
# 2. less than 60% of responses in English lexicon
criteria.English = 0.6
# 3. more than 20% of responses not unique (sex,sex,sex)
criteria.Repeat = 0.2
# 4. more than 30% of responses are multi-word
criteria.Ngram = 0.3
results      = list()
# Import the raw dataset
data.file    = './data/raw/SWOW-EN.complete.csv'
output.file  = './data/processed/SWOW-EN.R100.csv'
lexicon.file = './data/dictionaries/wordlist.txt'
cueCorrections.file = './data/dictionaries/cueCorrections.txt'
report.file  = './output/reports/preprocessing.SWOW-EN.report.rds'
X            = read.table(data.file, header = TRUE, sep = ",", dec = ".", quote = "\"",stringsAsFactors = FALSE,
encoding = 'UTF-8')
head(X)
Lexicon      = read.csv(lexicon.file, header = TRUE,stringsAsFactors = FALSE, encoding = 'UTF-8')
doubles = X %>% filter(R1 == R2, R1 != unknown.Token) %>% select(participantID,cue,R1,R2) %>% nrow()
doubles = doubles + X %>% filter(R2 == R3, R2 != missing.Token,R2 != unknown.Token) %>%
select(participantID,cue,R1,R2)  %>% nrow()
X       = X %>% mutate(R2 = ifelse(R1 == R2 & R1 != unknown.Token, missing.Token,R2))
X       = X %>% mutate(R3 = ifelse(R2 == R3 & R2 != missing.Token & R2 != unknown.Token, missing.Token,R3))
inconsistent        = X$R2 ==missing.Token & X$R3 != missing.Token
X$R2[inconsistent]  = X$R3[inconsistent]
X$R3[inconsistent]  = missing.Token
results$responses$doubles = doubles
message('Removed ',doubles, ' repeated responses for a single cue')
X           = gather(X,RPOS,response,R1,R2,R3,factor_key = FALSE)
results$responses$N.original = X %>% nrow()
message('Original number of responses: ', results$responses$N.original)
X           = X %>% mutate(isMissing = as.numeric(response == missing.Token))
X           = X %>% mutate(isUnknown = as.numeric(response  == unknown.Token))
X           = X %>% na_if(unknown.Token)
X           = X %>% na_if(missing.Token)
X$created_at = as.POSIXct(strptime(X$created_at, format = "%Y-%m-%d %H:%M:%S",tz ='UTC'))
spelling.words  = read.table(cueCorrections.file,sep = '\t',header=TRUE,stringsAsFactors = FALSE, encoding = 'UTF-8')
X$cue           = plyr::mapvalues(X$cue,spelling.words$original,spelling.words$correction,warn_missing = FALSE)
X               = X %>% mutate(nWords = ifelse(isMissing >  0, NA, ifelse(str_count(response,"\\S+") + str_count(response,",|;") > 1,1,0)))
X               = X %>%  mutate(inLexicon = ifelse(isMissing > 0, NA, as.numeric(response %in% Lexicon$Word)))
PP              = X %>% group_by(participantID,nativeLanguage,gender,age,education) %>%
summarise(N = n(),
Unknown = sum(isUnknown),
Missing = sum(isMissing),
C.Response = n_distinct(response),
F.English = sum(inLexicon,na.rm = TRUE),
F.words = sum(nWords,na.rm = TRUE))
PP$Prop.Unknown = PP$Unknown / PP$N
PP$Prop.Repeat  = 1 - (PP$C.Response - as.numeric(PP$Unknown>0) - as.numeric(PP$Missing>0)) / (PP$N - PP$Unknown - PP$Missing)
PP$Prop.X       = (PP$Missing + PP$Unknown) / PP$N
PP$Prop.English = PP$F.English / (PP$N - PP$Unknown - PP$Missing)
PP$Prop.Ngram   = PP$F.words / (PP$N - PP$Unknown - PP$Missing)
PP              = PP %>% mutate(Status = ifelse(Prop.X >  criteria.X, 'X',
ifelse(Prop.English < criteria.English,'Non-native',
ifelse(Prop.Repeat > criteria.Repeat, 'Perseveration',
ifelse(Prop.Ngram > criteria.Ngram, 'Verbose','Valid')))))
results$pp$N     = dim(PP)[1]
results$pp$N.invalid.X          = sum(PP$Prop.X > criteria.X)
results$pp$N.invalid.nonnative  = sum(PP$Prop.English < criteria.English, na.rm = TRUE)
results$pp$N.invalid.persever   = sum(PP$Prop.Repeat > criteria.Repeat, na.rm  = TRUE)
results$pp$N.invalid.ngram      = sum(PP$Prop.Ngram > criteria.Ngram, na.rm = TRUE)
results$pp$female    = PP %>% filter(gender=='Fe') %>% nrow()
results$pp$male      = PP %>% filter(gender=='Ma') %>% nrow()
results$pp$X         = PP %>% filter(gender=='X') %>% nrow()
results$pp$age.M     = round(mean(PP$age))
results$pp$age.SD    = round(sd(PP$age),1)
results$pp$N.native    = round(PP %>% filter(nativeLanguage %in% nativeLanguages) %>% nrow() / results$pp$N * 100)
results$pp$N.america   = round(PP %>% filter(nativeLanguage == 'United States') %>% nrow() / results$pp$N * 100)
results$pp$N.canada    = round(PP %>% filter(nativeLanguage == 'Canada') %>% nrow() / results$pp$N * 100)
results$pp$N.uk        = round(PP %>% filter(nativeLanguage == 'United Kingdom') %>% nrow() / results$pp$N * 100)
results$pp$N.australia = round(PP %>% filter(nativeLanguage == 'Australia') %>% nrow() / results$pp$N * 100)
results$pp$N.education = PP %>% group_by(education) %>% summarise(Freq = n())
nPP         = PP %>% group_by(Status) %>% summarise(Freq = n())
results$pp$N.invalid = round(100 * nPP %>% filter(!Status=='Valid') %>% summarise(Freq = sum(Freq)))
# Remove from data
X           = X %>% filter(participantID %in% PP$participantID[PP$Status=='Valid'])
# Verify sufficient responses in the set of cues after removal of invalid pp's
Cues          = X %>% group_by(cue) %>% summarise(Freq = n())
missing       = Cues %>% filter(Freq < responseCountTreshold) %>% arrange(Freq)
missing$Freq  = (responseCountTreshold - missing$Freq)/3
message('Number of participants missing: ', ceiling(sum(missing$Freq)/listlength.default))
X_wide    = X %>%  select(id,participantID,age,gender,nativeLanguage,country,education,created_at,cue,response,RPOS) %>% spread(RPOS,response)
# Add a selection variable to favor native speakers
X_wide    = X_wide %>% mutate(Native = ifelse(nativeLanguage == 'United States', 3,
ifelse(nativeLanguage %in% c('Canada','Australia','New Zealand','Jamaica','Puerto Rico'),2,
ifelse(nativeLanguage %in% c('Ireland','United Kingdom'), 1,0)))) %>%
arrange(participantID)
d = as.double(max(X_wide$participantID))
X_wide$SampleKey = X_wide$Native + (X_wide$participantID/d)
X_set     = X_wide %>% group_by(cue) %>% top_n(100,SampleKey) %>% arrange(participantID) %>%
select(id,participantID,age,gender,nativeLanguage,country,education,created_at,cue,R1,R2,R3)
results$responses$N.set100 =  X_set %>% nrow()
PP.set    = X_set %>% group_by(participantID,nativeLanguage,gender,age,education) %>%
summarise(N = n_distinct(participantID)) %>% nrow()
results$responses$N.valid = X_wide %>% nrow() * 3
results$responses$N.set100 = X_set %>% nrow() * 3
results$responses$N.valid - results$responses$N.set100
results$pp$N.set100  = X_set %>% group_by(participantID) %>% summarise(n_distinct(participantID)) %>% nrow()
write.csv(X_set,output.file)
saveRDS(results,report.file,ascii=TRUE)
require(igraph)
require(Matrix)
rm(list = ls())
results = list()
output.file         = './output/adjacencyMatrices/SWOW-EN'
report.file         = './output/reports/components.SWOW-EN.rds'
source('./R/functions/importDataFunctions.R')
source('./R/functions/networkFunctions.R')
source(settings)
source('settings.R')
require(igraph)
require(Matrix)
rm(list = ls())
results = list()
output.file         = './output/adjacencyMatrices/SWOW-EN'
report.file         = './output/reports/components.SWOW-EN.rds'
source('./R/functions/importDataFunctions.R')
getwd()
cd ..
setwd("/media/simon/Data/Dropbox/Scripts/R/SWOWGIT/SWOWEN-2018")
source('settings.R')
require(igraph)
require(Matrix)
rm(list = ls())
results = list()
output.file         = './output/adjacencyMatrices/SWOW-EN'
report.file         = './output/reports/components.SWOW-EN.rds'
source('./R/functions/importDataFunctions.R')
getwd()
setwd("/media/simon/Data/Dropbox/Scripts/R/SWOWGIT/SWOWEN-2018")
source('settings.R')
setwd("/media/simon/Data/Dropbox/Scripts/R/SWOWGIT/SWOWEN-2018/R")
require(igraph)
require(Matrix)
rm(list = ls())
results = list()
output.file         = './output/adjacencyMatrices/SWOW-EN'
report.file         = './output/reports/components.SWOW-EN.rds'
source('./R/functions/importDataFunctions.R')
getwd()
source('./functions/importDataFunctions.R')
source('./R/functions/importDataFunctions.R')
setwd("/media/simon/Data/Dropbox/Scripts/R/SWOWGIT/SWOWEN-2018/")
rm(list = ls())
results = list()
output.file         = './output/adjacencyMatrices/SWOW-EN'
report.file         = './output/reports/components.SWOW-EN.rds'
source('./R/functions/importDataFunctions.R')
source('./R/functions/networkFunctions.R')
setwd("/media/simon/Data/Dropbox/Scripts/R/SWOWGIT/SWOWEN-2018/")
rm(list = ls())
results = list()
output.file         = './output/adjacencyMatrices/SWOW-EN'
report.file         = './output/reports/components.SWOW-EN.rds'
source('./R/functions/importDataFunctions.R')
source('./R/functions/networkFunctions.R')
dataFile          = './data/processed/SWOW-EN.R100.csv'
response          = 'R1' # Options: R1, R2, R3 or R123
X.R1              = importDataSWOW(dataFile,response)
G.R1              = createGraph(X.R1)
compResults.R1    = extractComponent(G.R1,'strong')
G.R1.strong       = compResults.R1$subGraph
results$R1$removeVertices = compResults.R1$removedVertices
results$R1$maxSize = compResults.R1$maxSize
writeAdjacency(G.R1.strong, paste(output.file,response,sep=''))
response          = 'R123' # Options: R1, R2, R3 or R123
X.R123            = importDataSWOW(dataFile,response)
G.R123            = createGraph(X.R123)
compResults.R123  = extractComponent(G.R123,'strong')
G.R123.strong     = compResults.R1$subGraph
results$R123$removeVertices = compResults.R123$removedVertices
results$R123$maxSize = compResults.R123$maxSize
writeAdjacency(G.R123.strong, paste(output.file,response,sep=''))
saveRDS(results,report.file,ascii=TRUE)
source('./R/createSWOWENGraph.R')
source('settings.R')
source('./R/createSWOWENGraph.R')
getwd()
source('./R/createSWOWENGraph.R')
setwd("/media/simon/Data/Dropbox/Scripts/R/SWOWGIT/SWOWEN-2018/")
source('settings.R')
source('./R/createSWOWENGraph.R')
source('./R/createSWOWENGraph.R')
source('./R/createResponseStats.R')
source('./R/createCueStats.R')
head(cueStats)
head(cueStats.R1)
head(cueStats.R13)
head(cueStats.R123)
source('./R/plotCoverage.R')
source('./R/plotCoverage.R')
source('./R/plotVocabularyGrowth.R')
